

>RAII
- 资源获取即初始化，可以省掉很多异常逻辑的代码
>接口模式
- 一种设计模式，一种封装模式。实现类与接口类（纯虚类）分离的模式。隐藏不必要的成员，只暴露用户需要的接口。

&emsp;
## 1 RAII 与接口模式的封装
>infer.hpp
```c++
#ifndef INFER_HPP
#define INFER_HPP

#include <memory>
#include <string>

class InferInterface{
public:
    virtual void forward() = 0;
};

std::shared_ptr<InferInterface> create_infer(const std::string& file);

#endif // INFER_HPP
```

>infer.cpp
```c++
#include "infer.hpp"

using namespace std;

class InferImpl : public InferInterface{
public:
    // 加载模型
    bool load_model(const string& file){
        mContext = file;
        return true;
    }

    // 推理过程
    virtual void forward() override{

        printf("使用 %s 进行推理\n", mContext.c_str());
    }

    // 释放资源
    void destroy(){
        mContext.clear();
    }
private:
    string mContext;
};

// RAII 应用：获取 infer 实例，即表示加载模型
// 避免了外部调用 load_model
shared_ptr<InferInterface> create_infer(const string& file){
    shared_ptr<InferImpl> instance(new InferImpl());
    if(!instance->load_model(file))
        instance.reset();
    return instance;
}
```

>main.cpp
```c++
#include "infer.hpp"

using namespace std;

int main(int argc, char** argv)
{
    // 实例资源的获取并初始化
    auto infer = create_infer("model-a");
    if(!infer){
        printf("failed.\n");
        return -1;
    }
    infer->forward();
    return 0;
}
```

&emsp;
## 2 生产者与消费者，多 batch

>infer.hpp
```c++
#ifndef INFER_HPP
#define INFER_HPP

#include <string>
#include <future>
#include <memory>

// 封装接口类
class Infer{
public:
    virtual std::shared_future<std::string> commit(const std::string& input) = 0;
};

std::shared_ptr<Infer> create_infer(const std::string& file);

#endif // INFER_HPP
```

>infer.cpp
```c++
#include "infer.hpp"
#include <thread>
#include <vector>
#include <condition_variable>
#include <mutex>
#include <string>
#include <future>
#include <queue>
#include <functional>

// 封装接口类
using namespace std;

struct Job{
    shared_ptr<promise<string>> pro;
    string input;
};

class InferImpl : public Infer{
public:
    virtual ~InferImpl(){
        stop();
    }

    void stop(){
        if(running_){
            running_ = false;
            cv_.notify_one();
        }

        if(worker_thread_.joinable())
            worker_thread_.join();
    }

    bool startup(const string& file){

        file_ = file;
        running_ = true; // 启动后，运行状态设置为true

        // 线程传递promise的目的，是获得线程是否初始化成功的状态
        // 而在线程内做初始化，好处是，初始化跟释放在同一个线程内
        // 代码可读性好，资源管理方便
        promise<bool> pro;
        worker_thread_ = thread(&InferImpl::worker, this, std::ref(pro));
        /* 
            注意：这里thread 一构建好后，worker函数就开始执行了
            第一个参数是该线程要执行的worker函数，第二个参数是this指的是class InferImpl，第三个参数指的是传引用，因为我们在worker函数里要修改pro。
         */
        return pro.get_future().get();
    }

    virtual shared_future<string> commit(const string& input) override{
        Job job;
        job.input = input;
        job.pro.reset(new promise<string>());

        shared_future<string> fut = job.pro->get_future();
        {
            lock_guard<mutex> l(lock_);
            jobs_.emplace(std::move(job));
        }
        cv_.notify_one();
        return fut;
    }

    void worker(promise<bool>& pro){
        // load model
        if(file_ != "trtfile"){

            // failed
            pro.set_value(false);
            printf("Load model failed: %s\n", file_.c_str());
            return;
        }

        // load success
        pro.set_value(true); // 这里的promise用来负责确认infer初始化成功了

        vector<Job> fetched_jobs;
        while(running_){
            
            {
                unique_lock<mutex> l(lock_);
                cv_.wait(l, [&](){return !running_ || !jobs_.empty();}); // 一直等着，cv_.wait(lock, predicate) // 如果 running不在运行状态 或者说 jobs_有东西 而且接收到了notify one的信号

                if(!running_) break; // 如果 不在运行 就直接结束循环
                
                int batch_size = 5;
                for(int i = 0; i < batch_size && !jobs_.empty(); ++i){   // jobs_不为空的时候
                    fetched_jobs.emplace_back(std::move(jobs_.front())); // 就往里面fetched_jobs里塞东西
                    jobs_.pop();                                         // fetched_jobs塞进来一个，jobs_那边就要pop掉一个。（因为move）
                }
            }

            // 一次加载一批，并进行批处理
            // forward(fetched_jobs)
            for(auto& job : fetched_jobs){
                job.pro->set_value(job.input + "---processed");
            }
            fetched_jobs.clear();
        }
        printf("Infer worker done.\n");
    }

private:
    atomic<bool> running_{false};
    string file_;
    thread worker_thread_;
    queue<Job> jobs_;
    mutex lock_;
    condition_variable cv_;
};

shared_ptr<Infer> create_infer(const string& file){
    shared_ptr<InferImpl> instance(new InferImpl()); // 实例化一个推理器的实现类（inferImpl），以指针形式返回 
    if(!instance->startup(file)){                    // 推理器实现类实例(instance)启动。这里的file是engine file
        instance.reset();                            // 如果启动不成功就reset
    }
    return instance;    
}
```

>main.cpp
```c++

```